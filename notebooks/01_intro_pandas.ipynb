{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d338987c-d1ee-4c90-8f98-65dfd4fc4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813d7da0-e51f-4c7b-8327-438836ecbc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbfdd64-ac58-4963-9aaf-96db065d79a5",
   "metadata": {},
   "source": [
    "Pandas introduces two primary data structures: Series and DataFrame, along with an Index \n",
    "that labels tha data.\n",
    "Series: Its a one-dimensional array of data with an index (like a labeled column of values).\n",
    "DataFrame: DataFrame is a two-dimensional table of data, consisting of multiple Series \n",
    "that share the same index(like a spreedsheet or SQL table).\n",
    "The index is a set of labels for each row (and each column, in the case of column names),\n",
    "by default, Pandas assings an integer index starting from 0 for each row.\n",
    "Why do we use pandas?\n",
    "In real-world data analysis, you will oftern actually work with dataset\n",
    "( csv files, databases, JSON APIS, etc). That would need cleaning, transformation and summarization.\n",
    "Pandas provids high-levl data structures and functions that makes these tasks easy. \n",
    "Its built on Numpy, and it gives high performace of numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602b55a4-3075-4c13-9ec1-2a3679dbaea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series of Scores:\n",
      "Alice      89\n",
      "Bob        90\n",
      "Charlie    88\n",
      "Dana       88\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a Series of exam scores\n",
    "scores = pd.Series((89, 90, 88, 88), index=[\"Alice\", \"Bob\", \"Charlie\", \"Dana\"])\n",
    "print(\"Series of Scores:\")\n",
    "print(scores, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb79846b-20da-4cca-95a5-43c96f406194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of df: Index(['Alice', 'Bob', 'Charlie', 'Dana'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of df:\", scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be55f202-c78b-4275-b42d-3a2d8a5acd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age City  Score\n",
      "0    Alice   45   NY     89\n",
      "1      Bob   33   LA     90\n",
      "2  Charlie   55   NY     88\n",
      "3     Dana   22   TX     88\n"
     ]
    }
   ],
   "source": [
    "# Create a Dataframe of students with multiple columns\n",
    "data = {\n",
    "    \"Name\": [ \"Alice\", \"Bob\", \"Charlie\", \"Dana\" ],\n",
    "    \"Age\": [45, 33, 55, 22],\n",
    "    \"City\": [\"NY\", \"LA\", \"NY\", \"TX\"],\n",
    "    \"Score\": [89, 90, 88, 88]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baad255a-a450-49db-a7fa-7527550e7e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of df: Index(['Name', 'Age', 'City', 'Score'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns of df:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "059fd9a4-6873-4179-b0c4-a37b5d2984ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame read from CSV:\n",
      "      Name  Age City\n",
      "0    Alice   45   NY\n",
      "1      Bob   33   LA\n",
      "2  Charlie   55   NY\n",
      "3     Dana   22   TX \n",
      "\n",
      "DataFrame has been written to the file\n"
     ]
    }
   ],
   "source": [
    "# creating csv data ( comma seperated values )\n",
    "csv_data = \"\"\"Name,Age,City\n",
    "Alice,45,NY\n",
    "Bob,33,LA\n",
    "Charlie,55,NY\n",
    "Dana,22,TX\n",
    "\"\"\"\n",
    "# Use StringIo to simulate a file object from the string ( for demo purposes )\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "print(\"DataFrame read from CSV:\")\n",
    "print(df,\"\\n\")\n",
    "\n",
    "\n",
    "# Now write this Dataframe to an Excel file ( this will actually create an actual file 'people.xlsx'\n",
    "df.to_excel(\"people.xlsx\", index=False) # index=False to omits the index in the file\n",
    "\n",
    "print(\"DataFrame has been written to the file\")\n",
    "\n",
    "# In the code above pd.read_csv was used to parse CSV data. in practice, you would\n",
    "# pd.read_csv(\"path/to/your/file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "397ae099-e987-4024-8f71-f491f768e3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame read from JSON:\n",
      "  Name  Score\n",
      "0    X      5\n",
      "1    Y      7\n",
      "DataFrame has been written to 'sample.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6z/6f9t9fvd78ndfhydznyqv3vc0000gn/T/ipykernel_76655/1935835857.py:3: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_json = pd.read_json(json_data)\n"
     ]
    }
   ],
   "source": [
    "# JSON string example\n",
    "json_data = '[{\"Name\": \"X\", \"Score\": 5}, {\"Name\": \"Y\", \"Score\": 7}]'\n",
    "df_json = pd.read_json(json_data)\n",
    "print(\"DataFrame read from JSON:\")\n",
    "print(df_json)\n",
    "\n",
    "# Write DataFrame to JSON File\n",
    "df_json.to_json(\"Sample.json\", orient=\"records\")\n",
    "print(\"DataFrame has been written to 'sample.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431b809f-3142-4e0c-8a46-7ee39072f41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Product  Price  Quantity\n",
      "0  widget     23        33\n",
      "1  Gadget     34        44\n",
      "DataFrame has been written to products.csv\n"
     ]
    }
   ],
   "source": [
    "product_data = {\n",
    "    \"Product\": [\"widget\", \"Gadget\"],\n",
    "    \"Price\": [23,34],\n",
    "    \"Quantity\": [33,44]\n",
    "}\n",
    "df = pd.DataFrame(product_data)\n",
    "print(df)\n",
    "# Write DataFrame to csv file\n",
    "df.to_csv(\"products.csv\", index=False)\n",
    "print(\"DataFrame has been written to products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "495a3dd3-3ba4-4845-8799-2897f350db9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Product  Price  Quantity\n",
      "0  widget     23        33\n",
      "1  Gadget     34        44\n"
     ]
    }
   ],
   "source": [
    "# Reading from the csv file\n",
    "df = pd.read_csv(\"products.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebb18946-e1b8-4d49-b067-50955aacd6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age City\n",
      "0    Alice   45   NY\n",
      "1      Bob   33   LA\n",
      "2  Charlie   55   NY\n",
      "3     Dana   22   TX\n"
     ]
    }
   ],
   "source": [
    "# Reading from the excel file\n",
    "df = pd.read_excel(\"people.xlsx\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "612b0764-88e0-4ac0-8bc8-7a8d12a5ec5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Name  Score\n",
      "0    X      5\n",
      "1    Y      7\n"
     ]
    }
   ],
   "source": [
    "# Reading from the JSON file\n",
    "df = pd.read_json('Sample.json')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac4cfb-b069-4f4a-a24e-3458e01102ba",
   "metadata": {},
   "source": [
    "Once data is loaded into a DataFrame, the first step is to inspect it.\n",
    "Its shape, structure and basic statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a847ac-f759-49e3-89b7-7862c7be37ef",
   "metadata": {},
   "source": [
    "# This is for dataframe\n",
    "df.head(n) - view the first n rows ( default is 5)\n",
    "df.tail(n) - view the last n rows\n",
    "df.shape - get the number of rows and columns as tuple (n_rows, n_cols)\n",
    "df.columns - get the column labels\n",
    "df.index - get the index ( row labels)\n",
    "df.dtypes - data types of each column\n",
    "df.info() - concise summary: shows the index range, column names,\n",
    "            non-null counts, and dtypes.\n",
    "df.describe() - descriptive statics for numeric columns\n",
    "              (count, mean, std, min, quartiles, max)\n",
    "if we pass include='all', it will attempt to summarize non-numerical columns.\n",
    "(example count of unique, top value frequency)\n",
    "\n",
    "# series\n",
    "ser.value_counts() - frequency count of unique values\n",
    "ser.unique() - array of unique values\n",
    "ser.mean(), ser.min(), ser.max(), ser.sum(), ser.median() common statistics\n",
    "ser.isna()- Boolean series indicating missing values (Nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c838b-b13c-4668-baaa-4d128c8292f2",
   "metadata": {},
   "source": [
    "Whenever you get a new dataset, you should always perform an initial exploration.\n",
    "For example, if you're analyzing a dataset of customer purchases-\n",
    "How many records are there?\n",
    "What columns(features) does it have?\n",
    "Are they numeric, categorical, dates?\n",
    "Are there missing values to worry about?\n",
    "What are the ranges of typical values of numerical values ( describe())?\n",
    "Do any columns have suspicous values ( like negative values, or ages )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b99a537-1632-403e-bd2f-f633453d4d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frist 3 rows:\n",
      "       Name  Age City  Score\n",
      "0    Alice   45   NY     89\n",
      "1      Bob   33   LA     90\n",
      "2  Charlie   55   NY     88 \n",
      "\n",
      "DataFrame shape (4, 4)\n",
      "Columns: Index(['Name', 'Age', 'City', 'Score'], dtype='object')\n",
      "Data types:\n",
      " Name     object\n",
      "Age       int64\n",
      "City     object\n",
      "Score     int64\n",
      "dtype: object \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Name    4 non-null      object\n",
      " 1   Age     4 non-null      int64 \n",
      " 2   City    4 non-null      object\n",
      " 3   Score   4 non-null      int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 260.0+ bytes\n",
      "\n",
      "Summary statistics\n",
      "              Age      Score\n",
      "count   4.000000   4.000000\n",
      "mean   38.750000  88.750000\n",
      "std    14.338177   0.957427\n",
      "min    22.000000  88.000000\n",
      "25%    30.250000  88.000000\n",
      "50%    39.000000  88.500000\n",
      "75%    47.500000  89.250000\n",
      "max    55.000000  90.000000\n",
      "0    NY\n",
      "1    LA\n",
      "2    NY\n",
      "3    TX\n",
      "Name: City, dtype: object\n",
      "Unique cities: ['NY' 'LA' 'TX']\n",
      "Counts of each city:\n",
      " City\n",
      "NY    2\n",
      "LA    1\n",
      "TX    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Code Demo\n",
    "data = {\n",
    "    \"Name\": [ \"Alice\", \"Bob\", \"Charlie\", \"Dana\" ],\n",
    "    \"Age\": [45, 33, 55, 22],\n",
    "    \"City\": [\"NY\", \"LA\", \"NY\", \"TX\"],\n",
    "    \"Score\": [89, 90, 88, 88]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Frist 3 rows:\\n\", df.head(3),\"\\n\" )\n",
    "print(\"DataFrame shape\", df.shape)\n",
    "print(\"Columns:\", df.columns)\n",
    "print(\"Data types:\\n\",df.dtypes,\"\\n\")\n",
    "df.info() # prints info to console\n",
    "print(\"\\nSummary statistics\\n\",df.describe())\n",
    "print(df[\"City\"])\n",
    "print(\"Unique cities:\", df[\"City\"].unique())\n",
    "print(\"Counts of each city:\\n\", df[\"City\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc63f6-7f05-4d40-b3cf-e4d444bbfb0c",
   "metadata": {},
   "source": [
    "# Indexing, Selecting, Filtering, and Sorting Data\n",
    "\n",
    "Pandas actually offers multiple ways to index and select. \n",
    "Column Selection. you can select a column as a series using df[\"ColumnName\"]. \n",
    "To select multiple columns, you can pass a list: df[[\"ColA\", \"ColB\"]]\n",
    "It returns a new DataFrame with only those columns.\n",
    "\n",
    "Row selection: \n",
    "Using the .loc indexer. For  df.loc[0] returns the row with index label 0. \n",
    "If your DataFrame has a custom index [ say, one of the colums as a index), you use labels .loc\n",
    "allows you to slecte by row label and column label: df.loc[row_label, col_label]\n",
    "i can take slices as well (example,  df.loc[2:4] for labels 2 through 4, inclusive). \n",
    "\n",
    "Row selection by position:\n",
    "Using the .iloc indexer, example, df.iloc[0] returns the first row ( regardless of the index). df.iloc[0:3] returns first three rows (0,1,2). You can also do df.iloc[ [0,2], [1,3]] to get specific rows and columns by integer position.\n",
    "\n",
    "Boolean indexing (filtering):\n",
    "You can pass a boolean condition to the Dataframe to filter rows. for example: df[df[\"Age\"] > 30] returns only the rows wehre the Age column is greater than 30.\n",
    "You can also combine conditions with & (and) and | (or)  \n",
    "example, df[(df[\"City\"] == \"NY\") & (df[df[\"Score\"] > 80]) give me rows where city is NY and score> 80\n",
    "\n",
    "\n",
    "Sorting:\n",
    "Use df.sort_values(\"ColumnName\") to sort by a column (ascending by default)\n",
    "df.sort_values(\"ColumnName\", ascending=False) for descending.\n",
    "\n",
    "You can sort by multiple columns df.sort_values([\"City\", \"Age\"]) sort by City, then by age within each city. To sort by the index, use df.sort_index()\n",
    "\n",
    "\n",
    "Important things to know.\n",
    "Pandas indexing can be a little confusing at first because of the dual use of []. writing df[<conditon>] is shorthand for filtering rows by a condition (boolean indexing) while df[\"Col\"] selects a column. To avoid the ambiguity and pitfalls, It's ofter clearer to use .loc and .iloc for explicit indexing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28f500cc-0635-4552-a4d2-ff6384f4d093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   Name      5 non-null      object        \n",
      " 1   Dept      5 non-null      object        \n",
      " 2   Salary    5 non-null      int64         \n",
      " 3   HireDate  5 non-null      datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(1), object(2)\n",
      "memory usage: 292.0+ bytes\n",
      "dataframe: \n",
      "       Name       Dept  Salary   HireDate\n",
      "0    Alice         HR   60000 2019-05-01\n",
      "1      Bob         IT   75000 2021-07-15\n",
      "2  Charlie    Finance   80000 2020-09-30\n",
      "3    David         IT   70000 2022-01-10\n",
      "4      Eve  Marketing   65000 2018-03-20\n",
      "Names Series:\n",
      " 0      Alice\n",
      "1        Bob\n",
      "2    Charlie\n",
      "3      David\n",
      "4        Eve\n",
      "Name: Name, dtype: object \n",
      "\n",
      "Subset of DataFrame (Name and Salary:\n",
      "       Name  Salary\n",
      "0    Alice   60000\n",
      "1      Bob   75000\n",
      "2  Charlie   80000\n",
      "3    David   70000\n",
      "4      Eve   65000 \n",
      "\n",
      "First row using iloc:\n",
      " Name                      Alice\n",
      "Dept                         HR\n",
      "Salary                    60000\n",
      "HireDate    2019-05-01 00:00:00\n",
      "Name: 0, dtype: object \n",
      "\n",
      "Row with index label 2 using loc:\n",
      " Name                    Charlie\n",
      "Dept                    Finance\n",
      "Salary                    80000\n",
      "HireDate    2020-09-30 00:00:00\n",
      "Name: 2, dtype: object \n",
      "\n",
      "Employees with Salary > 70000:\n",
      "       Name     Dept  Salary   HireDate\n",
      "1      Bob       IT   75000 2021-07-15\n",
      "2  Charlie  Finance   80000 2020-09-30 \n",
      "\n",
      "Employees in IT Dept:\n",
      "     Name Dept  Salary   HireDate\n",
      "1    Bob   IT   75000 2021-07-15\n",
      "3  David   IT   70000 2022-01-10 \n",
      "\n",
      "IT employees with salary greater than 75000:\n",
      " Empty DataFrame\n",
      "Columns: [Name, Dept, Salary, HireDate]\n",
      "Index: [] \n",
      "\n",
      "Employees sorted by Salary (desc):\n",
      "       Name       Dept  Salary   HireDate\n",
      "2  Charlie    Finance   80000 2020-09-30\n",
      "1      Bob         IT   75000 2021-07-15\n",
      "3    David         IT   70000 2022-01-10\n",
      "4      Eve  Marketing   65000 2018-03-20\n",
      "0    Alice         HR   60000 2019-05-01 \n",
      "\n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "Name: Salary, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n",
    "    \"Dept\": [\"HR\", \"IT\", \"Finance\", \"IT\", \"Marketing\"],\n",
    "    \"Salary\": [60000, 75000, 80000, 70000, 65000],\n",
    "    \"HireDate\": pd.to_datetime(\n",
    "        [\"2019-05-01\", \"2021-07-15\", \"2020-09-30\", \"2022-01-10\", \"2018-03-20\"])\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.info()\n",
    "print(\"dataframe: \\n\",df.head())\n",
    "\n",
    "# Column selection\n",
    "name_series = df[\"Name\"]\n",
    "print(\"Names Series:\\n\", name_series, \"\\n\")\n",
    "\n",
    "subset = df[[\"Name\", \"Salary\"]]\n",
    "print(\"Subset of DataFrame (Name and Salary:\\n\", subset, \"\\n\")\n",
    "\n",
    "# Row selection by position\n",
    "\n",
    "first_row = df.iloc[0] # first row (as a series)\n",
    "print(\"First row using iloc:\\n\", first_row, \"\\n\")\n",
    "\n",
    "\n",
    "# Row selection by label (our index is 0,1,2,3...)\n",
    "row_label_2 = df.loc[2]\n",
    "print(\"Row with index label 2 using loc:\\n\", row_label_2, \"\\n\")\n",
    "\n",
    "# Filtering ( boolean indexing )\n",
    "\n",
    "high_salary = df[df[\"Salary\"] > 70000]\n",
    "print(\"Employees with Salary > 70000:\\n\", high_salary, \"\\n\")\n",
    "\n",
    "\n",
    "IT_employees = df[df[\"Dept\"] == \"IT\"]\n",
    "print(\"Employees in IT Dept:\\n\", IT_employees, \"\\n\")\n",
    "\n",
    "# Combined condition: IT and salary greater than 75000\n",
    "IT_high_salary_employee = df[(df[\"Dept\"] == \"IT\") & (df[\"Salary\"] > 75000)]\n",
    "print(\"IT employees with salary greater than 75000:\\n\", IT_high_salary_employee, \"\\n\")\n",
    "\n",
    "# Sorting\n",
    "sorting_by_salary = df.sort_values(\"Salary\", ascending=False)\n",
    "print(\"Employees sorted by Salary (desc):\\n\", sorting_by_salary,\"\\n\")\n",
    "\n",
    "\n",
    "# visual explanation of condition df for boolean\n",
    "print(df[\"Salary\"] > 800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282c141-a0ae-4520-ab49-6b6531fa992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {\n",
    "\"Name\": [\"Alice \", \"Bob\", \"Charlie\", \"Eve\"],\n",
    "\"Age\": [25, 30, 35, 40 ],\n",
    "\"City\": [\"NY\", \"LA\", \"NY\", \"LA\", ],\n",
    "\"Score\": [85, 90, 88, 75]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e048c62-8899-4a24-b7e5-02d00c31be3b",
   "metadata": {},
   "source": [
    "Using the DataFrame df or a dataset of your own:\n",
    "1. Select only the \"Name\" column\n",
    "using two different methods (as a Series, and as a one-column DataFrame). (Hint: df[\"Name\"] vs\n",
    "df[[ \"Name\" ]] .) \n",
    "2. Select the first two rows of the DataFrame using .iloc . \n",
    "3. Select the last row\n",
    "using .iloc (you can use -1 as the index in Python for last). \n",
    "4. Filter the DataFrame to only include rows\n",
    "where Age is greater than 30.\n",
    "5. Filter to include rows where City is \"NY\" or \"LA\" (use the | operator\n",
    "for OR). \n",
    "6. Sort the DataFrame by Score in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5b86e-0195-492c-ad70-c6102d897606",
   "metadata": {},
   "source": [
    " # Data Cleaning- Handling Missing Values and Data Types\n",
    "Real datasets are often messy: missing values, wrong types (e.g., numbers stored as strings), \n",
    "inconsitent formatting (caps, spaces), duplicates, etc). \n",
    "Pandas offers tools to clean and prep the data for analysis.\n",
    "\n",
    "Missing Values:\n",
    "Pandas uses NaN (Not a Number) or None as placeholders for missing data.\n",
    "key functions: df.isna() or df.isnull() - returns a DataFrame of booleans where True=missing.\n",
    "- df.notna() - opposite. - df.isna().sum()- sum of True by column gives count of NaNs in each column.\n",
    "- Dropping missing - df.dropna() - drops any row with NaN in any column (you can change with how='all'\n",
    "  to drop only if all columns are NaN, or axis=1 to drop columns with NaNs.)\n",
    "- Filling missing: df.fillna(value) - replace NaNs with a spcified value ( e.g, 0 or \"Unknown\").\n",
    "- We can also do df.fillna(method='ffil') to forward-fill (use last known value) of bfill for backward-fill\n",
    "  useful in time series.\n",
    "- You can fill with different values per column by passing a dict\n",
    "  df.fillna({\"Age\":df[\"Age].mean(), \"City\": \"Unknown\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d24af5d-79cb-4d84-af62-567729d168e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name  Score\n",
      "0  False  False\n",
      "1   True  False\n",
      "2  False   True\n",
      "Name     1\n",
      "Score    1\n",
      "dtype: int64\n",
      "      Name  Score\n",
      "0    Alice   95.0\n",
      "1  Unknown   85.0\n",
      "2  Charlie   90.0\n"
     ]
    }
   ],
   "source": [
    "df_miss = pd.DataFrame({\n",
    "    \"Name\":[\"Alice\", None, \"Charlie\"],\n",
    "    \"Score\": [95, 85, None]\n",
    "})\n",
    "print(df_miss.isna())\n",
    "print(df_miss.isna().sum()) # count Nans per column\n",
    "\n",
    "df_miss_filled = df_miss.fillna({\"Name\": \"Unknown\", \"Score\": df_miss[\"Score\"].mean()})\n",
    "print(df_miss_filled)\n",
    "# we replaced missing Name with \"Unknown\" and missing Score with mean score.\n",
    "\n",
    "\n",
    "# Caution:\n",
    "# filling numeric Nan with mean (as float) is fine; it was int column, it becomes float( Since NaN is float).\n",
    "# you can convert dtype later if needed ( df[\"Score\"] = df[\"Score\"].astype['Int64'] for nullable int type\n",
    "# that can hold NaN.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac6954-1fe2-4a10-8a3b-73cda3a8423f",
   "metadata": {},
   "source": [
    "# Data Type conversions\n",
    "\n",
    "- Check df.dtypes to see types.\n",
    "  Common cleaning:\n",
    "- Numbers read as strings -> convert numeric with pd.to_numeric(series, errors='coerce')\n",
    "  (will turn non-vertible to NaN) or series.astype(int/float) if you're sure all are numeric.\n",
    "- Strings that are actually dates -> pd.to_datetime(series, errors='coerce') to convert.\n",
    "- Categorical data( like entries repeating) -> you can convert to pandas \"category\" dtype to save memory or\n",
    "  to indicate it's categorical ( e.g, df[\"City\"] = df[\"City\"].astype('category')).\n",
    "\n",
    "  example: df_people[\"Age\"] = pd.to_numeric[df_people[\"Age\"], errors='raise')\n",
    "  If a non-numeric is found, errors='raise' would throw. errors='coerce' would set those to NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eebf6a-e0a6-4d1e-8d44-4b76d205ff5b",
   "metadata": {},
   "source": [
    "# String Operations\n",
    "\n",
    "Pandas Series have a .str accessor to apply string methods element-wise.\n",
    "\n",
    "- df[\"Name\"].str.lower()/upper()/title() - change case.\n",
    "- df[\"ID\"].str.strip() - remove any leading/trailling whitespace.\n",
    "- df[\"Email\"].str.contains(\"@gmail.com\") - Boolean Series if substring present.\n",
    "- df[\"City\"].str.replace(\"New York\", \"NYC\") - simple replacements ( can use regex too)\n",
    "- If you need to split strings: df[\"FullName\"].str.split(\" \",expand=True) gives a Dataframe with split parts.\n",
    "  example df_people[\"City\"] = df_people[\"City\"].str.title().str.strip()\n",
    "  This would make \" new york \" to \"New York\"\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3dec1-6c9d-4f65-a7c3-30ea7e32999b",
   "metadata": {},
   "source": [
    "# Removing The Duplicates\n",
    "\n",
    "- df.duplicated() - Boolean series, True for rows that are duplciates of previous row (can specify subset of cols)\n",
    "- df.drop_duplicates() - removes duplicate rows (keeping the first occurance by default).\n",
    "  e.g, If we had duplicate entries in df_people, df_people.drop_duplicates(subset=[\"Name\"], keep=\"first\")\n",
    "  drop later entries with same Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e75e972e-ffb6-4f90-abed-b1eeee0149b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:\n",
      "     Name Age         City\n",
      "0  Alice  24     new york\n",
      "1   Bob   27  Los Angeles\n",
      "2    bob  27  los angeles\n",
      "3   None  45      Chicago\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.DataFrame({\n",
    "    \"Name\": [\"Alice\", \"Bob \", \"bob\", None],\n",
    "    \"Age\": [\"24\", \"27\", \"27\", \"45\"],\n",
    "    \"City\": [\"new york\", \"Los Angeles\", \"los angeles\", \"Chicago\"]\n",
    "})\n",
    "print(\"Raw:\\n\", df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4df4dde7-3cbf-4b6a-a7cd-51bc6d3ae267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "# strip and title-case Name and City\n",
    "df[\"Name\"] = df[\"Name\"].str.strip().str.title()\n",
    "df[\"City\"] = df[\"City\"].str.strip().str.title()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79292346-71b7-4564-b4f7-c2b5add32001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned:\n",
      "       Name  Age         City\n",
      "0    Alice   24     New York\n",
      "1      Bob   27  Los Angeles\n",
      "3  Unknown   45      Chicago\n"
     ]
    }
   ],
   "source": [
    "# Handle Missing Name\n",
    "df[\"Name\"] = df[\"Name\"].fillna(\"Unknown\")\n",
    "\n",
    "# convert Age to numeric\n",
    "df[\"Age\"] = pd.to_numeric(df[\"Age\"], errors='coerce')\n",
    "\n",
    "# Drop the duplicates for Name (Keeping first)\n",
    "\n",
    "df = df.drop_duplicates(subset=\"Name\", keep='first')\n",
    "\n",
    "print(\"Cleaned:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdce356-3523-4ca1-aa5b-dbe9e4a8c72a",
   "metadata": {},
   "source": [
    "# Feature Engineering - Creating New Columns\n",
    "Feature engineering invloves transforming existing data or creating new features (columns that may be useful for analysis or modeling. The could be derived metrics, categorical encodings, flags.\n",
    "\n",
    "You can create new columns by assinging to df[\"NewCol\"]\n",
    "\n",
    "- df[\"TotalScore\"] = df[\"Score1\"] + df[\"Score2\"]\n",
    "- df[\"AvgScore\"] = df[[\"Score1\",\"Score2\"]].mean(axsis=1)\n",
    "\n",
    "if you apply a scalar operation:\n",
    "- df[\"Score1_pct\"] = df[\"Score1\"] / df[\"Score1\"].sum() * 100\n",
    "\n",
    "Conditonal columns\n",
    "\n",
    "- Using np.where or boolean logic:\n",
    "  For example, mark if a studnet passed both tests\n",
    "  - import numpy as np\n",
    "  - df[\"PassedBoth\"] = np.where((df[\"Score1\"]>=60) & (df[\"Score2\"]>=60), True, False)\n",
    "- where()\n",
    "- df[\"HonorRoll\"] == df[\"AvgScore\"].where(df[\"AvgScore\"]>=90, other=0) # keeps score if >90 else 0\n",
    "\n",
    "- You might also combine text\n",
    "  - df[\"fullName\"] = df[\"FirstName\"] + \" \" + df[\"lastName\"]\n",
    "- extract parts of a string\n",
    "  - df[\"Domain\"] = df[\"Email\"].str.split(\"@\", expand=True)[1]\n",
    "- Date example. if you have a datetime column\n",
    "  - df[\"Year\"]= df[\"Date\"].dt.year\n",
    "  - df[\"Month\"] = df[\"Date\"].dt.month_name()\n",
    "  - df[\"IsWeekend\"] = df[\"Date\"].dt.weekday >=5\n",
    "- Encoding the Categorical Variables\n",
    "  - Mapping to numerics. e.g, df[\"GenderCode\"] = df[\"Gender\"].map({\"Male\":1, \"Female\":0})\n",
    "  - one-hot encoding: turn categories into dummy/indicator columns\n",
    "    dummies = pd.get_dummies(df[\"Category\"], prefix=\"Cat\")\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    This will add columns Cat_A, Cat_B... with 1/0 indicating presence of category ( useful for\n",
    "    machine learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e28c8a-1be0-4cc3-b871-16761b537cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  BirthYear City  Age2025  IsNYC       NameCity\n",
      "0    Alice       1990  NYC       35      1    Alice (NYC)\n",
      "1      Bob       1985   LA       40      0       Bob (LA)\n",
      "2  Charlie       1992  NYC       33      1  Charlie (NYC)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "\"Name\": [\"Alice\",\"Bob\",\"Charlie\"],\n",
    "\"BirthYear\": [1990, 1985, 1992],\n",
    "\"City\": [\"NYC\",\"LA\",\"NYC\"]\n",
    "})\n",
    "# Add current age in 2025\n",
    "df[\"Age2025\"] = 2025 - df[\"BirthYear\"]\n",
    "# Flag if from NYC\n",
    "df[\"IsNYC\"] = df[\"City\"].apply(lambda x: 1 if x==\"NYC\" else 0)\n",
    "# combine name and city\n",
    "df[\"NameCity\"] = df[\"Name\"] + \" (\" + df[\"City\"] + \")\"\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c33b7a-efda-4d97-9447-bef9e761e7ac",
   "metadata": {},
   "source": [
    "# GroupBy and Aggregation - Summarizing Data\n",
    "It is essential for analyzing data by categories - simliar to SQL's GROUPBY.\n",
    "It involves splitting data into groups, applying an aggregate function, and combining results.\n",
    "\n",
    "Common Aggregations: sum, mean, min, max, median ,std , etc.\n",
    "\n",
    "- The GroupBy process.\n",
    "    - Split: Define groups based on one or more keys(columns). e.g. group by \"City\"\n",
    "    - Apply: Compute an aggreate for each group(sum of sales per city, average age per city, etc)\n",
    "    - combine: The results are returned in a new Series or DataFrame indexed by the group Key(s)\n",
    "- pandas syntax.\n",
    "      - df.groupby(\"Column\")[\"OtherColumn\"].function()\n",
    "  Note: if you omit the [\"OtherColumn\"]. It will attempt to aggregate all numeric columns by defualt.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a9fadd1-ddd3-43c5-9305-38f4023b51d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      City  Revenue  Quantity\n",
      "0       NY      100        30\n",
      "1       LA       80        20\n",
      "2       NY       90        25\n",
      "3       LA       70        30\n",
      "4  Chicago       60        15\n"
     ]
    }
   ],
   "source": [
    "sales = pd.DataFrame({\n",
    "    \"City\": [\"NY\",\"LA\",\"NY\",\"LA\",\"Chicago\"],\n",
    "    \"Revenue\": [100, 80, 90, 70, 60],\n",
    "    \"Quantity\": [30, 20, 25, 30, 15]\n",
    "})\n",
    "print(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c71da7d4-b345-4fe7-b97e-9a4b27e45da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City\n",
      "Chicago     60\n",
      "LA         150\n",
      "NY         190\n",
      "Name: Revenue, dtype: int64\n",
      "info of the groupby result <bound method Series.info of City\n",
      "Chicago     60\n",
      "LA         150\n",
      "NY         190\n",
      "Name: Revenue, dtype: int64>\n"
     ]
    }
   ],
   "source": [
    "# Group by City for the total revenue\n",
    "rev_by_city = sales.groupby(\"City\")[\"Revenue\"].sum()\n",
    "print(rev_by_city)\n",
    "\n",
    "print(\"info of the groupby result\",rev_by_city.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b3e34b-d425-41de-9994-2514428cc629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Revenue  Quantity\n",
      "City                      \n",
      "Chicago       60        15\n",
      "LA           150        50\n",
      "NY           190        55\n"
     ]
    }
   ],
   "source": [
    "# Multiple aggregations: sum of revenue and quantity by city\n",
    "agg_by_city = sales.groupby(\"City\").agg({\"Revenue\": \"sum\", \"Quantity\": \"sum\"})\n",
    "print(agg_by_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e98171a0-3b93-4778-90c6-1dc1dbec605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding another category column: \n",
      "       City  Revenue  Quantity Category\n",
      "0       NY      100        30        A\n",
      "1       LA       80        20        A\n",
      "2       NY       90        25        B\n",
      "3       LA       70        30        A\n",
      "4  Chicago       60        15        B\n",
      "\n",
      " After multi groupby \n",
      " City     Category\n",
      "Chicago  B            60\n",
      "LA       A           150\n",
      "NY       A           100\n",
      "         B            90\n",
      "Name: Revenue, dtype: int64\n",
      "\n",
      " Turned into df \n",
      "       City Category  Revenue\n",
      "0  Chicago        B       60\n",
      "1       LA        A      150\n",
      "2       NY        A      100\n",
      "3       NY        B       90\n",
      "\n",
      " Chicago ->       City  Revenue  Quantity Category\n",
      "4  Chicago       60        15        B 1 records\n",
      "\n",
      " LA ->   City  Revenue  Quantity Category\n",
      "1   LA       80        20        A\n",
      "3   LA       70        30        A 2 records\n",
      "\n",
      " NY ->   City  Revenue  Quantity Category\n",
      "0   NY      100        30        A\n",
      "2   NY       90        25        B 2 records\n"
     ]
    }
   ],
   "source": [
    "# you can also pass a list of functions\n",
    "sales.groupby(\"City\")[\"Revenue\"].agg([\"sum\", \"mean\", \"count\"])\n",
    "\n",
    "# Here since City is categorical datatype\n",
    "\n",
    "# For a single column group, you get Series output (as in rev_by_city).\n",
    "# If grouping by more than one key.\n",
    "\n",
    "# Example group by two keys (City and maybe another, but our data doesn't have\n",
    "# another category).\n",
    "# let's add a fake Category column for demonstration:\n",
    "\n",
    "sales[\"Category\"] = [\"A\", \"A\", \"B\", \"A\", \"B\"]\n",
    "print(\"After adding another category column: \\n\", sales)\n",
    "multi_group = sales.groupby([\"City\", \"Category\"])[\"Revenue\"].sum()\n",
    "print(\"\\n After multi groupby \\n\", multi_group)\n",
    "# This is a multi-index Series. You can .reset_index() to turn it to a Falt DF\n",
    "multi_group_df = multi_group.reset_index()\n",
    "print(\"\\n Turned into df \\n\", multi_group_df)\n",
    "\n",
    "# we can also iterate over df.groupby(\"City\")\n",
    "\n",
    "for city, group in sales.groupby(\"City\"):\n",
    "    # print(\"\\n\",group)\n",
    "    print(\"\\n\", city, \"->\", group, len(group), \"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86120d82-fecb-43e0-a1b8-2354da9129c3",
   "metadata": {},
   "source": [
    "# Combining DataFrames - merges and join\n",
    " - use pd.merge(df1, df3, how='inner', on='KeyColumn')\n",
    " - alternatively df1.merge(df2, how='left', left_on='key1', right_on=\"key_2\")\n",
    "\n",
    "    - join types:\n",
    "    - inner join: only matching keys in both(default)\n",
    "    - left join: all keys from left, add data from right when keys match\n",
    "      ( unmatched get NaN on right cols).\n",
    "    - right join: opposite of left.\n",
    "    - Outer join: all keys from both, NaN where no match on either side\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed02adb3-b8eb-4ec7-ae55-c4fcc717be91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers:\n",
      "    CustomerID     Name\n",
      "0           1    Alice\n",
      "1           2      Bob\n",
      "2           3  Charlie\n",
      "Orders:\n",
      "    OrderId  CustomerID  Amount\n",
      "0      101           2     250\n",
      "1      102           2     100\n",
      "2      103           3      50\n",
      "3      104           4     300\n"
     ]
    }
   ],
   "source": [
    "df_customers = pd.DataFrame({\n",
    "    \"CustomerID\": [1, 2, 3],\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"]\n",
    "})\n",
    "df_orders = pd.DataFrame({\n",
    "    \"OrderId\": [101, 102, 103, 104],\n",
    "    \"CustomerID\": [2,2,3,4],\n",
    "    \"Amount\": [250, 100, 50, 300]\n",
    "})\n",
    "\n",
    "print(\"Customers:\\n\", df_customers)\n",
    "print(\"Orders:\\n\", df_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb620bb2-35cc-44e7-98c1-383907ec0bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner join on CustomerID:\n",
      "    CustomerID     Name  OrderId  Amount\n",
      "0           2      Bob      101     250\n",
      "1           2      Bob      102     100\n",
      "2           3  Charlie      103      50\n"
     ]
    }
   ],
   "source": [
    "# Merge customers with orders on CustomerID:\n",
    "merged = pd.merge(df_customers, df_orders, how=\"inner\", on=\"CustomerID\")\n",
    "print(\"Inner join on CustomerID:\\n\", merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918398d-8629-4ccf-888c-55167a080c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
